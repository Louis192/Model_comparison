{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this project I will be implementing the  comparison of 3 different kNN models on 3 different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "K-Nearest Neighbors (k-NN) was first proposed by Fix and Hodges in 1951 [1] and has become one of the most widely used supervised learning algorithms in today‚Äôs world[2]. Unlike the other algorithms such as support vector machine and decision tree, k-NN does not require any model training before making the prediction and thus is also known as a lazy classifier. \n",
    "The algorithm assumed that similar data points exist in close proximity and thus classification of new data points can be made based on the label of their nearest neighbors.\n",
    "k-NN has several advantages that make it important in the machine learning area. First, the process is transparent and thus makes it easy to implement, debug and understand.\n",
    "Second, it is a non-parametric algorithm, which does not require any data distribution assumption. \n",
    "Third, it does not require a training phase, thus the model training time can be neglected, and new data points can be added seamlessly without affecting the model. \n",
    "Because of these advantages, kNNs have been successful in various problems, including weather forecasting, Distributed Denial-of-Service (DDoS) detection, and recommender system (see part II for details). \n",
    "Despite the above advantages, the downsides of k-NN include 1) difficult to determine a suitable distance metric and k-value; 2) difficult to apply and poor run-time performance in big data settings due to the high time and computation cost in the implementation phase (i.e. require to re-calculate and store the distances between all training data points for each query)[3]; 3) sensitive to irrelevant data features as all features are used to calculate the distance[4]. \n",
    "To overcomes these disadvantages, advanced k-NN algorithms have been developed, including filter-and-refinement, KD tree, R tree, and Locality Sensitivity Hashing(LSH), which will be discussed in detail in part III."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Three big data problems which can benefit from k-NN search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Weather forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Climate change has become one of the global challenges for human in the 21st century[5]. According to a recent World Meteorological Organization (WMO) report, over 2 million deaths and $3.64 trillion US dollars were costed annually by hazardous weather events[6]. Thus, accurate and timely weather prediction is desired to facilitate the government to prepare for disasters and minimize the number of casualties and financial losses.\n",
    "Weather predictions are made by analyzing the high volume of data collected from a range of specialized devices, including radar, thermometers, rain gauges, barometers, and anemometers in multiple observation sites. The collected data were then input into the conventional weather prediction models, which are mainly numerical- or statistical-based. Despite the reasonable accuracy of the prediction, the conventional weather prediction models are subject to an extremely long computation time and expensive computational resources due to the formula complexity and high volume of data[7]. The conventional methods are also highly sensitive to errors in the input dataset[8] and the results are difficult for laymen to interpret.\n",
    "To overcome the limitations of the conventional methods, various k-NN based weather prediction model was proposed and has been repeatedly proved to have better performance than the conventional models[9-12].\n",
    "The basic idea of k-NN-based weather forecasting is an analogous approach to predict the weather on the interest day by selecting a specified number(k) of days that have similar characteristics (e.g. temperature, precipitation, solar radiance) from the historical records. According to Sharif and Burn [13], the algorithm consists of the following steps: (1) consider there is a daily historic dataset consisting of p weather variables for N years (e.g. 30 years) and the first simulation day is day t with p weather data ; (2) determine the temporal window (e.g. 7 days prior and 7 days after the day of interest) and all days within the window for N years are regarded as the potential neighbors to the current feature vector. The size of the data block L can be\n",
    "expressed as L = (w+1) x N -1; (3) Determine a heuristic k value by using ùëò = ùêø as suggested by Yates, et al. [14]; (4) Calculate the distances between the feature vector of the simulation day and that of all days within the temporal window for N years. (5) Sort and select the first k nearest neighbors and then assign weight to\n",
    "each of the j neighbors according to the probability distribution: ùëù = 1/ùëó ; (6) Based on the calculated ùëóùêæ\n",
    "‚àë 1/ùëñ ùëñ=1\n",
    "probability metric, the values for the day subsequent to the selected nearest neighbor are adopted as the simulated values for day t+1. (7) Repeat the above steps 4 to 6 to predict the subsequent days.\n",
    " \n",
    " Various modified k-NN approaches have been developed and implemented on weather datasets in different regions [13, 15, 16]. Shankarnarayan and Ramakrishna [12] compared the k-NN approach with another two weather forecast approaches (Soil and Water Assessment Tools and Representative Concentration Pathway) and concluded that the k-NN approach has the best performance and is able to predict up to 17 weather features, including mean, minimum and maximum temperature, precipitation, solar radiance, and gust. Another study compared the performance of Na√Øve Bayes, K Nearest Neighbor, and C.45 method in weather forecast and found that k-NN is the best model with an accuracy of over 71% [17]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Distributed Denial-of-Service Attack (DDoS) Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Internet has become an inseparable part of our daily life and network security has become more important than ever before. Among all kinds of cyberattacks, distributed DDoS attack is the biggest threat. Famous companies, including the Hong Kong Stock Exchange [18], Paypal [19], Google[20] had been attacked by DDoS and leading to different degrees of financial loss[21]. In a DDoS attack, the attacker manipulates multiple machines to send massive spiteful requests to the system aiming to deplete the network bandwidth, router processing capacity, or the victim‚Äôs resource (e.g. disk or bandwidth, memory, and CPU cycles) so that the system cannot respond to the services requested by other normal users.\n",
    "Rule-based and anomaly-based approaches have been proposed to detect DDoS attack[22]. The rule-based approach utilizes a set of specific rules to detect abnormal data transmission. The rules are generated based on the behavior of normal traffic and the features and patterns of previous attacks. Although this approach can detect known attacks effectively, its ability to detect novel or modified attacks is limited. It also takes a relatively long time for rule matching and requires frequent updates of the rules. The anomaly-based approach can identify the novel attack as it works by comparing the incoming traffic with the normal traffic pattern. When there is a significant deviation between the incoming and normal traffic patterns, the detection system will produce an alarm.\n",
    "k-NN-based detection belongs to the anomaly-based approach. The detection system has a training dataset that contains the normal traffic and attack status data. The system first collects the features of the incoming traffic and then calculates and ranks the similarity between the incoming traffic and instances in the training dataset. Finally, the class (normal or attack) of the incoming traffic is determined by its k-nearest neighbors. The features of the dataset and the incoming traffic can be the transmission control protocol (TCP) basic features, TCP content features, and time-based and host-based network traffic features. Su [23] demonstrated that the accuracies of real-time DDoS detection by the k-NN-based method reached 97% and 78% for known and unknown types of attack, respectively. A recent study compared the performance of\n",
    "\n",
    " different Machine Learning algorithms on DDoS attacks detection and illustrated that the KNN-based method has comparable high performance (98.6%) and short test time and neglectable training time[24]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the recommender system grows along with the popularity of e-commerce. For example, Netflix estimated that its recommender system helps them save more than 1 billion annually[25] and Amazon revealed that 35% of what their customers purchased comes from the recommendations[26]. From the customer‚Äôs perspective, a good recommender system can help them to narrow down the number of products of interest when facing a massive number of choices. Based on the explicit (e.g. ratings) or implicit (e.g. purchase records) feedback, the system provides high-quality and personalized recommendations for the customers.\n",
    "Building an effective recommender system is a big data problem: Netflix, for instance, has over 200 million subscribers and over 15,000 movies and videos. One of the most widely used methods is the kNN algorithm[27] and it can be further divided into the user-based and the item-based approach. In the user-based kNN approach, it is assumed that if the users shared similar interests, for example, similar movie watching histories, it is very likely that the user will be interested in an unseen movie that was watched by other users with the same taste. The four major steps of user-based kNN involved: 1) calculate the similarities between the user of interest (U) and all other users based on their ratings on the same items. The similarity can be measured by Pearson correlation; 2) weight the users based on the similarity and select the top k similar users, as the nearest neighbors; 3) predict U‚Äôs ratings on a set of items of interest by calculating the weighted combination of normalized ratings by the selected users; 4) recommend the items with the highest estimated ratings to U[28]. The item-based kNN approach is well-known for its application in Amazon‚Äôs recommender system[29] and instead of the similarity between users, the similarity between items is used as the basis for the recommendation. The item-to-item similarity can be estimated from the user‚Äôs ratings, product descriptions, or the co-occurrence of the items in the user's shopping cart or history. To predict a user‚Äôs rating on an unseen item I, we first calculate the similarity of ratings between item I and other items (usually cosine similarity is used). The k items with the highest similarities are then selected and the prediction is made by using the weighted sum of the ratings of the other similar items[28].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Representative k-NN search algorithms and detailed procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force is the most basic approach of the kNN algorithm. The algorithm can be described as following [30]:\n",
    "\n",
    "(1)Calculate the Euclidean distance between the test point and all the training points.\n",
    "(2)Select the k-nearest neighbors based on the distance values.\n",
    "(3)Determine the majority class or average value of the k-nearest neighbors.\n",
    "(4)Predict the class or value for the test point based on the majority class or average value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KD Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KD Tree is constructed by first partitioning at the median point in any dimension and regarding that as the root of the tree, and then recursively dividing data on the second dimension, and so on(Fig x). This process stops until only one element is left on each side of the branch[31, 32].\n",
    "To search the nearest neighbor of a query point q (x‚Äô, y‚Äô):\n",
    "Step 1: Compare the first dimension value of q with the root x value, move right in the tree if q‚Äôs x value is greater than the root‚Äôs x value, otherwise left\n",
    "Step 2: Compare the second dimension value of q with the root and move\n",
    "Step 3: Repeat until a leaf node is found, and consider it as the closest-so-far (Fig 1)\n",
    "Step 4: Walk back up the tree, and check if the dividing hyperplane is closer than the closest-so-far. If yes, the other side of the tree must be searched.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ball Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ball tree is a binary tree with a hierarchical structure, each layer has 2 clusters (resembling balls/ hypersphere multidimensionally) representing nodes.\n",
    "\n",
    "Below are the steps for Ball tree algorithm\n",
    "1. Calculate the centroid of all data points.\n",
    "2. Set the point with the maximum distance to the centroid as the center of the first cluster (cluster A) and set the point furthest away from the center of the first cluster as the center of the second cluster (cluster B)\n",
    "3. Assign data points to either cluster A or B to its closest center.\n",
    "4. Determine a new centroid within each cluster\n",
    "5. For each cluster, repeat steps 2-4 until a defined depth is reached.\n",
    "The nearest neighbor search in the ball tree starts from the root and the process maintains a max-first priority queue Q of the k nearest neighbors. Let the query point be q, and at any node N, the algorithm performs the following steps:\n",
    "1) If dist (q, N) is greater than the distance between the furthest point in Q, ignore N\n",
    "2) If N is a leaf node, calculate the distance between q and all data points in N to find if there are any\n",
    "nearest neighbors, and update Q appropriately.\n",
    "3) If N is an internal node, first determine which child node is closer to q and then repeat the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Discussion, Advantages and disadvantages of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-force approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most basic form of the kNN search. The searches could be very competitive and efficient for small data samples [30]. However, as the number of samples grows, the brute-force approach quickly becomes infeasible. Its advantages include: 1) the algorithm is apparent ‚Äì making it easier for the user to understand, implement, and debug; 2) this approach is model-free (known as a lazy classifier) ‚Äì as it does not have a model training phase before making predictions and thus the model training time can be neglected; 3) it is a non-parametric method ‚Äì does not have any data distribution assumption. Its disadvantages include: 1) slow when the sample increases ‚Äì as for each query, the distances between the query point and all data points needed to be calculated; 2) difficult to determine a suitable k and distance metric; 3) sensitive to noise especially when the k is small; 4) performance might be affected if there are many irrelevant attributes in the dataset as these attributes might distort the distances between the query point and the training data points; 5) unable to handle imbalance class ‚Äì if the sizes of the classes are largely different(e.g. 99% in class A and 1% in class B), the algorithm tends to bias to the larger class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KD Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KD Tree is easy to build, and the computation of a nearest neighbors search can be efficient with aggregate distance information for the sample encoded. As it involves only partitioning along the data axes, no D-dimensional distances need to be computed, thus the construction of a KD tree is very fast[37]. Though\n",
    "the search is very fast for low-dimensional (D<20) neighbors searches, it becomes inefficient (and often slower than brute-force searches) as D grows as the radius of the nearest neighbour would intersect many partitions[37]. It is axis-aligned and may lose the balance guarantee which is necessary for strict complexity proofs for some operations. It allows adding elements dynamically despite it is suboptimal to construct the original tree this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ball Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ball Tree can speed up the discovery of neighborhood points efficiently especially if the number of d is huge[38]. Calculating the distance of query point from all the points in the n-dimensional space is not necessary, hastening up the classification process once nested hyperspheres are created and placed in memory. However, ball tree formation initially requires a lot of time and memory. And training is dependent on datapoint amount, dimensionality, and structure. Many data points of noise can lead to a suboptimal performance due to no clear structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Ball Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1649951492772,
     "user": {
      "displayName": "Peggy Chan",
      "userId": "11359610036216747305"
     },
     "user_tz": -480
    },
    "id": "hXgtYUF71bZb"
   },
   "outputs": [],
   "source": [
    "##import libraries\n",
    "# LIBSVM  is an integrated software that support vector classification,regression and distribution estimation,\n",
    "# it supports multi-class classification and it supports almost all programming languages interfaces.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from joblib import Memory\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import MNIST dataset\n",
    "\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/mnist.scale.tr.bz2\"           \n",
    "test= \"/Portfolio/Projects/BigData/mnist.scale.test.bz2\"           \n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    #Training dataset have 60000rows and 780 columns\n",
    "X_test = X_test.toarray()  #Test dataset have 10000rows and 778 columns\n",
    "\n",
    "##Create two columns in test dataset and set them to zeros \n",
    "cols = np.zeros((10000, 2))\n",
    "X_test = np.c_[X_test, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1649951273429,
     "user": {
      "displayName": "Peggy Chan",
      "userId": "11359610036216747305"
     },
     "user_tz": -480
    },
    "id": "ukiiwhU3vCQK",
    "outputId": "3bc32aba-0ce4-4cd2-a120-43a46c966c36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 780)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1649951277013,
     "user": {
      "displayName": "Peggy Chan",
      "userId": "11359610036216747305"
     },
     "user_tz": -480
    },
    "id": "ieaXybquvG3b",
    "outputId": "a437c215-e6d5-42bf-e81b-080fc7235745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 780)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - ball tree approach\n",
      "Accuracy: 0.97050\n",
      "Response time: 428.968\n"
     ]
    }
   ],
   "source": [
    "#ball tree approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='ball_tree', leaf_size=1000)\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "y_pred=clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - ball tree approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, y_pred))\n",
    "print('Response time: %.3f' % response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuttle dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/shuttle.scale.tr\"  \n",
    "test= \"/Portfolio/Projects/BigData/shuttle.scale.t\"  \n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    \n",
    "X_test = X_test.toarray()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - ball tree approach\n",
      "Accuracy: 0.99890\n",
      "Response time: 2.089\n"
     ]
    }
   ],
   "source": [
    "#ball tree approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='ball_tree', leaf_size=200)\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "ball_y_pred = clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - ball tree approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, ball_y_pred))\n",
    "print('Response time: %.3f' % response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ijcnn1 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/ijcnn1.tr.bz2\"\n",
    "test= \"/Portfolio/Projects/BigData/ijcnn1.t.bz2\"\n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    \n",
    "X_test = X_test.toarray()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - ball tree approach\n",
      "Accuracy: 0.96539\n",
      "Response time: 44.558\n"
     ]
    }
   ],
   "source": [
    "#ball tree approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='ball_tree', leaf_size=200)\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "ball_y_pred=clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - ball tree approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, ball_y_pred ))\n",
    "print('Response time: %.3f' % response_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Brute Force Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import MNIST dataset\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/mnist.scale.tr.bz2\"           \n",
    "test= \"/Projects/BigData/mnist.scale.test.bz2\" \n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    #Training dataset have 60000rows and 780 columns\n",
    "X_test = X_test.toarray()  #Test dataset have 10000rows and 778 columns\n",
    "\n",
    "##Create two columns in test dataset and set them to zeros \n",
    "cols = np.zeros((10000, 2))\n",
    "X_test = np.c_[X_test, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (60000, 780)\n",
      "Test dataset:  (10000, 780)\n",
      "number of classes in training dataset:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "number of classes in training dataset:  [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset: \", X_tr.shape)\n",
    "print(\"Test dataset: \", X_test.shape)\n",
    "print(\"number of classes in training dataset: \", np.unique(y_tr))\n",
    "print(\"number of classes in training dataset: \", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - brute force approach\n",
      "Accuracy: 0.97050\n",
      "Response time: 22.917\n"
     ]
    }
   ],
   "source": [
    "#brute force approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='brute', n_jobs=-1)\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "y_pred=clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)\n",
    "print('Evaluation of model performance - brute force approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, y_pred ))\n",
    "print('Response time: %.3f' % response_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuttle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/shuttle.scale.tr\"  \n",
    "test= \"/Portfolio/Projects/BigData/shuttle.scale.t\"  \n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    \n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (30450, 9)\n",
      "Test dataset:  (14500, 9)\n",
      "number of classes in training dataset:  [1. 2. 3. 4. 5. 6. 7.]\n",
      "number of classes in training dataset:  [1. 2. 3. 4. 5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset: \", X_tr.shape)\n",
    "print(\"Test dataset: \", X_test.shape)\n",
    "print(\"number of classes in training dataset: \", np.unique(y_tr))\n",
    "print(\"number of classes in training dataset: \", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - brute force approach\n",
      "Accuracy: 0.99890\n",
      "Response time: 6.172\n"
     ]
    }
   ],
   "source": [
    "#brute force approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='brute')\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "bf_y_pred = clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - brute force approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, bf_y_pred  ))\n",
    "print('Response time: %.3f' % response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ijcnn1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/ijcnn1.tr.bz2\"\n",
    "test= \"/Portfolio/Projects/BigData/ijcnn1.t.bz2\"\n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    \n",
    "X_test = X_test.toarray()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (35000, 22)\n",
      "Test dataset:  (91701, 22)\n",
      "number of classes in training dataset:  [-1.  1.]\n",
      "number of classes in training dataset:  [-1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset: \", X_tr.shape)\n",
    "print(\"Test dataset: \", X_test.shape)\n",
    "print(\"number of classes in training dataset: \", np.unique(y_tr))\n",
    "print(\"number of classes in training dataset: \", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - brute force approach\n",
      "Accuracy: 0.96539\n",
      "Response time: 47.277\n"
     ]
    }
   ],
   "source": [
    "#brute force approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='brute' )\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "b_y_pred=clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - brute force approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, b_y_pred ))\n",
    "print('Response time: %.3f' % response_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KD Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import MNIST dataset\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/mnist.scale.tr.bz2\"           \n",
    "test= \"/Portfolio/Projects/BigData/mnist.scale.test.bz2\" \n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    #Training dataset have 60000rows and 780 columns\n",
    "X_test = X_test.toarray()  #Test dataset have 10000rows and 778 columns\n",
    "\n",
    "##Create two columns in test dataset and set them to zeros \n",
    "cols = np.zeros((10000, 2))\n",
    "X_test = np.c_[X_test, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - kd tree approach\n",
      "Accuracy: 0.97050\n",
      "Response time: 465.427\n"
     ]
    }
   ],
   "source": [
    "#kd tree approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='kd_tree', leaf_size=200)\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "kd_y_pred = clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - kd tree approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, kd_y_pred  ))\n",
    "print('Response time: %.3f' % response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuttle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from joblib import Memory\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/shuttle.scale.tr\"  \n",
    "test= \"/Portfolio/Projects/BigData/shuttle.scale.t\"  \n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    \n",
    "X_test = X_test.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - kd tree approach\n",
      "Accuracy: 0.99890\n",
      "Response time: 0.418\n"
     ]
    }
   ],
   "source": [
    "#kd tree approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='kd_tree', leaf_size=200)\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "kd_y_pred = clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - kd tree approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, kd_y_pred  ))\n",
    "print('Response time: %.3f' % response_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ijcnn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "mem = Memory(\"./mycache\")\n",
    "\n",
    "tr = \"/Portfolio/Projects/BigData/ijcnn1.tr.bz2\"\n",
    "test= \"/Portfolio/Projects/BigData/ijcnn1.t.bz2\"\n",
    "\n",
    "@mem.cache\n",
    "def get_data(locat):\n",
    "    data = load_svmlight_file(locat)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X_tr, y_tr = get_data(tr)\n",
    "X_test, y_test = get_data(test)\n",
    "\n",
    "##Convert to array\n",
    "X_tr = X_tr.toarray()    \n",
    "X_test = X_test.toarray()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of model performance - kd tree approach\n",
      "Accuracy: 0.96539\n",
      "Response time: 19.229\n"
     ]
    }
   ],
   "source": [
    "#kdtree approach kNN\n",
    "clf=KNeighborsClassifier(n_neighbors=3,p=2,weights='uniform',algorithm='kd_tree', leaf_size=200)\n",
    "clf.fit(X_tr,y_tr)  \n",
    "\n",
    "timestamp = time.time()  #start response time counting\n",
    "kd_y_pred=clf.predict(X_test) \n",
    "response_time = time.time()- timestamp #stop response time counting\n",
    "\n",
    "## response time & other evaluation metrics for multi-label classification (mnist & shuttle)##\n",
    "print('Evaluation of model performance - kd tree approach')\n",
    "print('Accuracy: %.5f' % accuracy_score(y_test, kd_y_pred ))\n",
    "print('Response time: %.3f' % response_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='tableauPlaceholder' id='viz1678612573609' style='position: relative'><noscript><a href='#'><img alt='Response Time Of KNN Algorithms  ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Bo&#47;Book1_16786124122100&#47;Dashboard2&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Book1_16786124122100&#47;Dashboard2' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Bo&#47;Book1_16786124122100&#47;Dashboard2&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-GB' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1678612573609');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else { vizElement.style.width='100%';vizElement.style.height='727px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div class='tableauPlaceholder' id='viz1678612573609' style='position: relative'><noscript><a href='#'><img alt='Response Time Of KNN Algorithms  ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Bo&#47;Book1_16786124122100&#47;Dashboard2&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Book1_16786124122100&#47;Dashboard2' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Bo&#47;Book1_16786124122100&#47;Dashboard2&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-GB' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1678612573609');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else { vizElement.style.width='100%';vizElement.style.height='727px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above diagram shows the response time of brute force, KD tree, and ball tree approach in all three datasets.\n",
    "Among all datasets,the accuracies of using all three approaches are similar as there accuracies are above 96%.And this is to all three approaches are based on the kNN algorithm and by controlling the k =3 and the use of major voting in all algorithms, 3\n",
    "exact nearest neighbors are selected and thus the reaching the same accuracy.\n",
    "For response time, the average response time for the shuttle dataset is the shortest as its dimension is\n",
    "the least (9 features), while the response time for Mnist dataset is the longest as it has 780 features. KD tree works best in the Shuttle and ijcnn1 dataset (22 features) as the dimensions of both are not too big. KD tree can effectively reduce the number of possible candidates for comparison. Surprisingly, the brute force approach works better in the Mnist dataset than Ijcnn1, I suspected that it might be due to suboptimal leaf node size selections, high-dimension dataset, and the presence of noise points and irrelevant features in the dataset[40]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. References :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[1] B. W. Silverman and M. C. Jones, \"E. Fix and J.L. Hodges (1951): An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation: Commentary on Fix and Hodges (1951),\" International Statistical Review / Revue Internationale de Statistique, vol. 57, no. 3, pp. 233-238, 1989, doi: 10.2307/1403796.\n",
    " [2] X. Wu et al., \"Top 10 algorithms in data mining,\" Knowl. Inf. Syst., vol. 14, no. 1, pp. 1‚Äì37, 2007, doi: 10.1007/s10115-007-0114-2.\n",
    "[3] H. Li, T. N. Chan, M. L. Yiu, and N. Mamoulis, \"FEXIPRO: fast and exact inner product retrieval in recommender systems,\" in Proceedings of the 2017 ACM International Conference on Management of Data, 2017, pp. 835-850.\n",
    "[4] P. Cunningham and S. J. Delany, \"k-Nearest neighbour classifiers-A Tutorial,\" ACM Computing Surveys (CSUR), vol. 54, no. 6, pp. 1-25, 2021.\n",
    "[5] P. Lynch, \"The origins of computer weather prediction and climate modeling,\" Journal of computational physics, vol. 227, no. 7, pp. 3431-3444, 2008.\n",
    "[6] W. M. Organization, \"The Atlas of Mortality and Economic Losses from Weather, Climate and Water Extremes (1970‚Äì2019),\" ed: WMO-n¬∞ 1267; World Meteorological Organization Geneva, Switzerland, 2021.\n",
    "[7] U. M. Fayyad, S. G. Djorgovski, and N. Weir, \"Automating the analysis and cataloging of sky surveys,\" in Advances in knowledge discovery and data mining, 1996, pp. 471-493.\n",
    "[8] J. Han, J. Pei, and M. Kamber, Data mining: concepts and techniques. Elsevier, 2011.\n",
    "[9] Z. Jan, M. Abrar, S. Bashir, and A. M. Mirza, \"Seasonal to inter-annual climate prediction using data\n",
    "mining KNN technique,\" in International Multi Topic Conference, 2008: Springer, pp. 40-51.\n",
    "[10] M. Abrar, A. T. H. Sim, D. Shah, and S. Khusro, \"Weather Prediction Using Classification,\" Science\n",
    "International, vol. 26, no. 5, 2014.\n",
    "[11] H. Parvin, H. Alizadeh, and B. Minati, \"A modification on k-nearest neighbor classifier,\" Global Journal\n",
    "of Computer Science and Technology, 2010.\n",
    "[12] V. K. Shankarnarayan and H. Ramakrishna, \"Comparative study of three stochastic future weather\n",
    "forecast approaches: a case study,\" Data Science and Management, vol. 3, pp. 3-12, 2021.\n",
    "[13] M. Sharif and D. Burn, \"Development and application of a K-NN Weather Generating Model,\" Project\n",
    "Report III, UW, Waterloo, ON, 2004.\n",
    "[14] D. Yates, S. Gangopadhyay, B. Rajagopalan, and K. Strzepek, \"A technique for generating regional\n",
    "climate scenarios using a nearest-neighbor algorithm,\" Water Resources Research, vol. 39, no. 7, 2003.\n",
    "\n",
    " [15] M. Sharif, D. H. Burn, and K. M. Wey, \"Daily and hourly weather data generation using a k-nearest neighbour approach,\" in Canadian Hydrotechnical Conference, 2007: CHC Winnipeg, pp. 1-10.\n",
    "[16] B. Rajagopalan and U. Lall, \"A k-nearest-neighbor simulator for daily precipitation and other weather variables,\" Water resources research, vol. 35, no. 10, pp. 3089-3101, 1999.\n",
    "[17] Y. Findawati, I. I. Astutik, A. Fitroni, I. Indrawati, and N. Yuniasih, \"Comparative analysis of Na√Øve Bayes, K Nearest Neighbor and C. 45 method in weather forecast,\" in Journal of Physics: Conference Series, 2019, vol. 1402, no. 6: IOP Publishing, p. 066046.\n",
    "[18] E. Yiu, \"Hong Kong Stock Exchange says its website was hacked while it halted derivatives trading to fix unrelated software bug,\" in South China Morning Post, ed. Hong Kong SAR, 2019\n",
    "[19] B. news, \"Anonymous hackers 'cost PayPal ¬£3.5m',\" in BBC news, ed, 2012.\n",
    "[20] T. E. Times, \"Google stops biggest-ever DDoS cyber attack to date \" in The Economic Times, ed, 2020.\n",
    "[21] R. R. Nuiaa, S. Manickam, and A. H. Alsaeedi, \"Distributed reflection denial of service attack: A critical review,\" International Journal of Electrical & Computer Engineering (2088-8708), vol. 11, no. 6, 2021.\n",
    "[22] P. Kaur, M. Kumar, and A. Bhandari, \"A review of detection approaches for distributed denial of service attacks,\" Systems Science & Control Engineering, vol. 5, no. 1, pp. 301-320, 2017.\n",
    "[23] M.-Y. Su, \"Real-time anomaly detection systems for Denial-of-Service attacks by weighted k-nearest-neighbor classifiers,\" Expert Systems with Applications, vol. 38, no. 4, pp. 3492-3498, 2011.\n",
    "[24] F. Musumeci, A. C. Fidanci, F. Paolucci, F. Cugini, and M. Tornatore, \"Machine-Learning-Enabled DDoS\n",
    "Attacks Detection in P4 Programmable Networks,\" Journal of Network and Systems Management, vol.\n",
    "30, no. 1, pp. 1-27, 2022.\n",
    "[25] C. A. Gomez-Uribe and N. Hunt, \"The netflix recommender system: Algorithms, business value, and\n",
    "innovation,\" ACM Transactions on Management Information Systems (TMIS), vol. 6, no. 4, pp. 1-19,\n",
    "2015.\n",
    "[26] I. MacKenzie, C. Meyer, and S. Noble, \"How retailers can keep up with consumers,\" McKinsey &\n",
    "Company, vol. 18, p. 1, 2013.\n",
    "[27] D. H. Park, H. K. Kim, I. Y. Choi, and J. K. Kim, \"A literature review and classification of recommender\n",
    "systems research,\" Expert systems with applications, vol. 39, no. 11, pp. 10059-10072, 2012.\n",
    "[28] D. Jannach, M. Zanker, A. Felfernig, and G. Friedrich, Recommender systems: an introduction.\n",
    "Cambridge University Press, 2010.\n",
    "\n",
    "[29] G. Linden, B. Smith, and J. York, \"Amazon. com recommendations: Item-to-item collaborative filtering,\" IEEE Internet computing, vol. 7, no. 1, pp. 76-80, 2003.\n",
    "[30] P.-N. Tan, M. Steinbach, V. Kumar, and A. Karpatne, Introduction to Data Mining EBook: Global Edition. Harlow, UNITED KINGDOM: Pearson Education, Limited, 2019.\n",
    "[31] H. Marius. \"Tree algorithms explained: Ball Tree lgorithm vs. KD Tree vs. Brute Force.\"\n",
    "https://towardsdatascience.com/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-fo\n",
    "rce-9746debcd940 (accessed 5 Apr, 2022).\n",
    "[32] J. L. Bentley, \"Multidimensional binary search trees used for associative searching,\" Communications\n",
    "of the ACM, vol. 18, no. 9, pp. 509-517, 1975.\n",
    "[33] H. Saini. \"Introduction to R-tree.\" GeeksforGeeks.\n",
    "https://www.geeksforgeeks.org/introduction-to-r-tree/ (accessed 5 Apr, 2022).\n",
    "[34] S. M. Omohundro, Five balltree construction algorithms. International Computer Science Institute\n",
    "Berkeley, 1989.\n",
    "[35] A. Andoni and P. Indyk, \"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High\n",
    "Dimensions,\" in 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06),\n",
    "21-24 Oct. 2006 2006, pp. 459-468, doi: 10.1109/FOCS.2006.49.\n",
    "[36] S. Tan, \"An effective refinement strategy for KNN text classifier,\" Expert Systems with Applications,\n",
    "vol. 30, no. 2, pp. 290-298, 2006.\n",
    "[37] W. Hou, D. Li, C. Xu, H. Zhang, and T. Li, \"An advanced k nearest neighbor classification algorithm\n",
    "based on KD-tree,\" in 2018 IEEE International Conference of Safety Produce Informatization (IICSPI),\n",
    "2018: IEEE, pp. 902-905.\n",
    "[38] N. Bhatia, \"Survey of nearest neighbor techniques,\" arXiv preprint arXiv:1007.0085, 2010.\n",
    "[39] O. Jafari, K. M. Islam, and P. Nagarkar, \"Drawbacks and Proposed Solutions for Real-time Processing\n",
    "on Existing State-of-the-art Locality Sensitive Hashing Techniques,\" arXiv preprint arXiv:1912.07091,\n",
    "2019.\n",
    "[40] P. Cunningham and S. J. Delany, \"k-Nearest Neighbour Classifiers - A Tutorial,\" ACM Comput. Surv., vol.\n",
    "54, no. 6, p. Article 128, 2021, doi: 10.1145/3459665.\n",
    "https://ieeexplore.ieee.org/document/8731380\n",
    "https://dl.acm.org/doi/epdf/10.1145/2505515.2505749"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO/Zj1krr+qj6ZJ+Fh0c8Re",
   "collapsed_sections": [],
   "name": "kNN implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
